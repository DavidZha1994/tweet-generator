{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_path='dataset/'\n",
    "File_name='train_cleaned'\n",
    "with open(f'{File_path+File_name}.txt') as f:\n",
    "    text1 = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_path='dataset/'\n",
    "File_name='test_cleaned'\n",
    "text = []\n",
    "with open(f'{File_path+File_name}.txt') as f:\n",
    "    text2 = f.readlines()\n",
    "text1+=text2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in text1:\n",
    "    print(type(item))\n",
    "    \n",
    "    item.replace('&amp','')\n",
    "\n",
    "    text.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_table = pd.DataFrame(text,columns=['text'])\n",
    "print(titles_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "\n",
    "wc = wordcloud.WordCloud(\n",
    "            width=900,\n",
    "            height=900,\n",
    "            background_color='white')\n",
    "wc.generate(text)\n",
    "wc.to_file(f'wordcloud/Musk.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import collections\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#filtered_words = [word for word in all_headlines.split() if word not in stopwords]\n",
    "counted_words = collections.Counter([word for word in text.split()])\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(100):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "colors = cm.rainbow(np.linspace(0, 1, 10))\n",
    "rcParams['figure.figsize'] = 20, 20\n",
    "\n",
    "plt.title('Top words in the training dataset vs their count')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.barh(words, counts, color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "#titles_table = pd.read_csv(\"../input/ForumTopics.csv\")\n",
    "\n",
    "def wordCloudFunction(df,column,numWords):\n",
    "    topic_words = [ z.lower() for y in\n",
    "                       [ x.split() for x in df[column] if isinstance(x, str)]\n",
    "                       for z in y]\n",
    "    word_count_dict = dict(Counter(topic_words))\n",
    "    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n",
    "    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n",
    "    word_string=str(popular_words_nonstop)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='white',\n",
    "                          max_words=numWords,\n",
    "                          width=1000,height=1000,\n",
    "                         ).generate(word_string)\n",
    "    plt.clf()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def wordBarGraphFunction(df,column,title):\n",
    "    topic_words = [ z.lower() for y in\n",
    "                       [ x.split() for x in df[column] if isinstance(x, str)]\n",
    "                       for z in y]\n",
    "    word_count_dict = dict(Counter(topic_words))\n",
    "    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n",
    "    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n",
    "    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n",
    "    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "wordBarGraphFunction(titles_table,'text',\"Popular Words in Kaggle Forum Titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "wordCloudFunction(titles_table,'text',10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=model_name)\n",
    "result = sentiment_task([\"Entertainment will be critical when cars drive themselves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DL/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task twitter-roberta-base-sentiment, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text2text-generation', 'token-classification', 'translation', 'zero-shot-classification', 'zero-shot-image-classification', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/yu/Library/CloudStorage/OneDrive-个人/RUB/Semester_4/Master-Praktikum Deep Learning and Natural Language Processing/code/wordcloud1.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yu/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/RUB/Semester_4/Master-Praktikum%20Deep%20Learning%20and%20Natural%20Language%20Processing/code/wordcloud1.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yu/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/RUB/Semester_4/Master-Praktikum%20Deep%20Learning%20and%20Natural%20Language%20Processing/code/wordcloud1.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sentiment_pipeline \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtwitter-roberta-base-sentiment\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yu/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/RUB/Semester_4/Master-Praktikum%20Deep%20Learning%20and%20Natural%20Language%20Processing/code/wordcloud1.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#data = text#[\"I love you\", \"I hate you\"]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yu/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/RUB/Semester_4/Master-Praktikum%20Deep%20Learning%20and%20Natural%20Language%20Processing/code/wordcloud1.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mEntertainment will be critical when cars drive themselves\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/pipelines/__init__.py:523\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     task \u001b[39m=\u001b[39m get_task(model, use_auth_token)\n\u001b[1;32m    522\u001b[0m \u001b[39m# Retrieve the task\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m targeted_task, task_options \u001b[39m=\u001b[39m check_task(task)\n\u001b[1;32m    524\u001b[0m \u001b[39mif\u001b[39;00m pipeline_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     pipeline_class \u001b[39m=\u001b[39m targeted_task[\u001b[39m\"\u001b[39m\u001b[39mimpl\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/pipelines/__init__.py:369\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[39mreturn\u001b[39;00m targeted_task, (tokens[\u001b[39m1\u001b[39m], tokens[\u001b[39m3\u001b[39m])\n\u001b[1;32m    367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid translation task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m\u001b[39m format\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 369\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown task \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m, available tasks are \u001b[39m\u001b[39m{\u001b[39;00mget_supported_tasks() \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mtranslation_XX_to_YY\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task twitter-roberta-base-sentiment, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text2text-generation', 'token-classification', 'translation', 'zero-shot-classification', 'zero-shot-image-classification', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"twitter-roberta-base-sentiment\")\n",
    "#data = text#[\"I love you\", \"I hate you\"]\n",
    "data = [\"Entertainment will be critical when cars drive themselves\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "twitter-roberta-base-sentiment is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/configuration_utils.py:601\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m    602\u001b[0m         config_file,\n\u001b[1;32m    603\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    604\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    605\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    606\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    607\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    608\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    609\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    612\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/utils/hub.py:283\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    282\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    284\u001b[0m         url_or_filename,\n\u001b[1;32m    285\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    286\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    287\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    288\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    289\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    290\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    291\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    294\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/utils/hub.py:494\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    493\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mhead(url, headers\u001b[39m=\u001b[39mheaders, allow_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, proxies\u001b[39m=\u001b[39mproxies, timeout\u001b[39m=\u001b[39metag_timeout)\n\u001b[0;32m--> 494\u001b[0m _raise_for_status(r)\n\u001b[1;32m    495\u001b[0m etag \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Linked-Etag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/utils/hub.py:416\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m:\n\u001b[1;32m    415\u001b[0m     \u001b[39m# The repo was not found and the user is not Authenticated\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(\n\u001b[1;32m    417\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m401 Client Error: Repository not found for url: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    421\u001b[0m response\u001b[39m.\u001b[39mraise_for_status()\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error: Repository not found for url: https://huggingface.co/twitter-roberta-base-sentiment/resolve/main/config.json. If the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/yu/Library/CloudStorage/OneDrive-个人/RUB/Semester_4/Master-Praktikum Deep Learning and Natural Language Processing/code/wordcloud1.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yu/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/RUB/Semester_4/Master-Praktikum%20Deep%20Learning%20and%20Natural%20Language%20Processing/code/wordcloud1.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yu/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/RUB/Semester_4/Master-Praktikum%20Deep%20Learning%20and%20Natural%20Language%20Processing/code/wordcloud1.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sentiment_task \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msentiment-analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtwitter-roberta-base-sentiment\u001b[39;49m\u001b[39m\"\u001b[39;49m, tokenizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtwitter-roberta-base-sentiment\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yu/Library/CloudStorage/OneDrive-%E4%B8%AA%E4%BA%BA/RUB/Semester_4/Master-Praktikum%20Deep%20Learning%20and%20Natural%20Language%20Processing/code/wordcloud1.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sentiment_task(\u001b[39m\"\u001b[39m\u001b[39mCovid cases are increasing fast!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/pipelines/__init__.py:541\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(config, revision\u001b[39m=\u001b[39mrevision, _from_pipeline\u001b[39m=\u001b[39mtask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m    540\u001b[0m \u001b[39melif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 541\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(model, revision\u001b[39m=\u001b[39;49mrevision, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    543\u001b[0m model_name \u001b[39m=\u001b[39m model \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:680\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    679\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 680\u001b[0m config_dict, _ \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    681\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    682\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/configuration_utils.py:553\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    552\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    555\u001b[0m \u001b[39m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfiguration_files\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL/lib/python3.9/site-packages/transformers/configuration_utils.py:613\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m    602\u001b[0m         config_file,\n\u001b[1;32m    603\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         user_agent\u001b[39m=\u001b[39muser_agent,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    612\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 613\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    614\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier listed on \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token having \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    616\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpermission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    617\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    620\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    621\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists for this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    622\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel name. Check the model page at \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mavailable revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: twitter-roberta-base-sentiment is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=\"twitter-roberta-base-sentiment\", tokenizer=\"twitter-roberta-base-sentiment\")\n",
    "sentiment_task(\"Covid cases are increasing fast!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b5da4dccc5d959110f70bd428b51197bb0688003461a0e87be372a9c01e32ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
